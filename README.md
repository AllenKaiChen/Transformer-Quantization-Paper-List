# :mag_right:Transformer-Quantization-Paper-List

> Transformer has achieved great success in natural language processing (NLP) tasks and computer vision (CV) tasks over the past two years, showing great potential to surpass CNN in an increasing number of scenarios. However, transformer's special structure and large number of parameters limit its application on resource-constrained devices. More and more research involves how to optimize the transformer model so that it can actually be deployed in more scenarios。

> **:speaker:This repository aims to collect the latest transformer model optimization papers, especially those on quantization.**

## :bookmark:Table of Contents
- [:page_with_curl:Papers](#papers)




## Papers
- **:arrow_forward:[[arxv 2022](https://arxiv.org/abs/2201.07703)]** Q-ViT: Fully Differentiable Quantization for Vision Transformer. **Institute of Automation, Chinese Academy of Sciences**
- **:arrow_forward:[[arxiv 2021](https://arxiv.org/pdf/2111.12293.pdf)]** PTQ4ViT: Post-Training Quantization Framework for Vision Transformers. **Peking University**
- **:arrow_forward:[[arxv 2021](https://arxiv.org/pdf/2111.13824.pdf)]** FQ-ViT: Fully Quantized Vision Transformer without Retraining. **MEGVII Technology**
- **:arrow_forward:[[arxv 2021](https://arxiv.org/abs/2109.12948)]**  Understanding and Overcoming the Challenges of Efficient Transformer Quantization **Qualcomm AI Research**
- **:arrow_forward:[[arxv 2021](https://arxiv.org/abs/2109.15082)]** Towards Efficient Post-training Quantization of Pre-trained Language Models **The Chinese University of Hong Kong**
- **:arrow_forward:[[PMLR 2021](http://proceedings.mlr.press/v139/kim21d/kim21d.pdf)]** I-BERT: Integer-only BERT Quantization **University of California, Berkeley**
- **:arrow_forward:[[NeurIPS2021](https://arxiv.org/abs/2106.14156)]** Post-Training Quantization for Vision Transformer. **Peking University and  Noah’s Ark Lab**
- **:arrow_forward:[[arxv 2020](https://arxiv.org/abs/2009.07453)]** Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation. **Samsung Research**




